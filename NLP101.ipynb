{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoieOf59aNlRkgH5Q2QSI8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aeleraqi/NLP101/blob/main/NLP101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **تثبيت المكتبات الضرورية**"
      ],
      "metadata": {
        "id": "nlGd4puDWZFl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rvqank5GELR"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "Uv9TJJp5JzLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-crfsuite"
      ],
      "metadata": {
        "id": "bRNRQplvKLh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "sdjxQR9kPaYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U gensim"
      ],
      "metadata": {
        "id": "aKsv653_EBTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "XeNvK7MaOEw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the 'punkt' resource\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "J4uiNrcFKPbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the 'averaged_perceptron_tagger' resource\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "CFvLAxYtOcmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "brm44J6zT7fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **النص محل التطبيق**"
      ],
      "metadata": {
        "id": "xltSjCpzWMK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Arabic text\n",
        "arabic_text = \"تعرض اللاعب محمد الشناوي لاعب النادي الأهلي لإصابة شديدة الخطورة، نقل على أثرها إلى المستشفى، لتلقي العلاج الطبي، لكن تعافى قبل أن يصل إلى المستشفى! في تاريخ 09/12/2023\""
      ],
      "metadata": {
        "id": "5xpcaP6HGFhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **تجزئة النص لوحدات صغيرة**"
      ],
      "metadata": {
        "id": "1w3yMLDGWia5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using nltk\n",
        "tokens = word_tokenize(arabic_text)\n",
        "print(\"Tokenization:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHVtBzuTHYoi",
        "outputId": "8272d942-fdd8-4c20-ee50-997eea07205a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:\n",
            "['تعرض', 'اللاعب', 'محمد', 'الشناوي', 'لاعب', 'النادي', 'الأهلي', 'لإصابة', 'شديدة', 'الخطورة،', 'نقل', 'على', 'أثرها', 'إلى', 'المستشفى،', 'لتلقي', 'العلاج', 'الطبي،', 'لكن', 'تعافى', 'قبل', 'أن', 'يصل', 'إلى', 'المستشفى', '!', 'في', 'تاريخ', '09/12/2023']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(arabic_text)\n",
        "print(\"Tokenization:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTdAqyo5D3rt",
        "outputId": "1d751e18-9c8e-4da7-fa8c-cc9b1ee070db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:\n",
            "['تعرض', 'اللاعب', 'محمد', 'الشناوي', 'لاعب', 'النادي', 'الأهلي', 'لإصابة', 'شديدة', 'الخطورة،', 'نقل', 'على', 'أثرها', 'إلى', 'المستشفى،', 'لتلقي', 'العلاج', 'الطبي،', 'لكن', 'تعافى', 'قبل', 'أن', 'يصل', 'إلى', 'المستشفى', '!', 'في', 'تاريخ', '09/12/2023']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **تحديد الكلمات الأكثر تكرارًا**"
      ],
      "metadata": {
        "id": "OOchRF2fWm0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word frequencies\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "arabic_text = \"تعرض اللاعب محمد الشناوي لاعب النادي الأهلي لإصابة شديدة الخطورة، نقل على أثرها إلى المستشفى، لتلقي العلاج الطبي، لكن تعافى قبل أن يصل إلى المستشفى! في تاريخ 09/12/2023\"\n",
        "\n",
        "# Find the most frequent word\n",
        "most_freq_word = word_freq.most_common(2)\n",
        "\n",
        "print(\"Most frequent word:\")\n",
        "print(most_freq_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFgFXYCBKqot",
        "outputId": "ae4287a0-73aa-4608-fbef-018175a5649b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent word:\n",
            "[('إلى', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **تحديد علامات الترقيم في النص**"
      ],
      "metadata": {
        "id": "nKYQm4qSWrin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify punctuation marks\n",
        "punctuation_marks = [token for token in tokens if token in string.punctuation]\n",
        "\n",
        "arabic_text = \"تعرض اللاعب محمد الشناوي لاعب النادي الأهلي لإصابة شديدة الخطورة، نقل على أثرها إلى المستشفى، لتلقي العلاج الطبي، لكن تعافى قبل أن يصل إلى المستشفى! في تاريخ 09/12/2023\"\n",
        "\n",
        "print(\"Punctuation marks:\")\n",
        "print(punctuation_marks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_nucD_BLehR",
        "outputId": "a70ba59d-30c8-471f-e248-e6baa92e6476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation marks:\n",
            "['!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **تحديد أسماء الكيانات في النص**"
      ],
      "metadata": {
        "id": "DtG73GVbWvsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.29 datasets==2.14.5 sentencepiece==0.1.99"
      ],
      "metadata": {
        "id": "zwOZIlIISi4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import logging\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_3l-zBYSv1y",
        "outputId": "0d36ab5c-707d-4682-d657-8fb2847ee205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cp = \"marefa-nlp/marefa-ner\"\n",
        "custom_labels = [\"O\", \"B-job\", \"I-job\", \"B-nationality\", \"B-person\", \"I-person\", \"B-location\",\n",
        "                 \"B-time\", \"I-time\", \"B-event\", \"I-event\", \"B-organization\", \"I-organization\",\n",
        "                 \"I-location\", \"I-nationality\", \"B-product\", \"I-product\", \"B-artwork\", \"I-artwork\"]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_cp)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_cp, num_labels=len(custom_labels))"
      ],
      "metadata": {
        "id": "XB_QPiKpMEgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _extract_ner(text: str, model: AutoModelForTokenClassification,\n",
        "                 tokenizer: AutoTokenizer, start_token: str=\"▁\"):\n",
        "    tokenized_sentence = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    tokenized_sentences = tokenized_sentence['input_ids'].numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokenized_sentence)\n",
        "\n",
        "    last_hidden_states = output[0].numpy()\n",
        "    label_indices = np.argmax(last_hidden_states[0], axis=1)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_sentences[0])\n",
        "    special_tags = set(tokenizer.special_tokens_map.values())\n",
        "\n",
        "    grouped_tokens = []\n",
        "    for token, label_idx in zip(tokens, label_indices):\n",
        "        if token not in special_tags:\n",
        "            if not token.startswith(start_token) and len(token.replace(start_token,\"\").strip()) > 0:\n",
        "                grouped_tokens[-1][\"token\"] += token\n",
        "            else:\n",
        "                grouped_tokens.append({\"token\": token, \"label\": custom_labels[label_idx]})\n",
        "\n",
        "    # extract entities\n",
        "    ents = []\n",
        "    prev_label = \"O\"\n",
        "    for token in grouped_tokens:\n",
        "        label = token[\"label\"].replace(\"I-\",\"\").replace(\"B-\",\"\")\n",
        "        if token[\"label\"] != \"O\":\n",
        "\n",
        "            if label != prev_label:\n",
        "                ents.append({\"token\": [token[\"token\"]], \"label\": label})\n",
        "            else:\n",
        "                ents[-1][\"token\"].append(token[\"token\"])\n",
        "\n",
        "        prev_label = label\n",
        "\n",
        "    # group tokens\n",
        "    ents = [{\"token\": \"\".join(rec[\"token\"]).replace(start_token,\" \").strip(), \"label\": rec[\"label\"]}  for rec in ents ]\n",
        "\n",
        "    return ents"
      ],
      "metadata": {
        "id": "LYQB7CplNMKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _extract_ner(text: str, model: AutoModelForTokenClassification,\n",
        "                 tokenizer: AutoTokenizer, start_token: str=\"▁\"):\n",
        "    tokenized_sentence = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    tokenized_sentences = tokenized_sentence['input_ids'].numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokenized_sentence)\n",
        "\n",
        "    last_hidden_states = output[0].numpy()\n",
        "    label_indices = np.argmax(last_hidden_states[0], axis=1)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_sentences[0])\n",
        "    special_tags = set(tokenizer.special_tokens_map.values())\n",
        "\n",
        "    grouped_tokens = []\n",
        "    for token, label_idx in zip(tokens, label_indices):\n",
        "        if token not in special_tags:\n",
        "            if not token.startswith(start_token) and len(token.replace(start_token,\"\").strip()) > 0:\n",
        "                grouped_tokens[-1][\"token\"] += token\n",
        "            else:\n",
        "                grouped_tokens.append({\"token\": token, \"label\": custom_labels[label_idx]})\n",
        "\n",
        "    # extract entities\n",
        "    ents = []\n",
        "    prev_label = \"O\"\n",
        "    for token in grouped_tokens:\n",
        "        label = token[\"label\"].replace(\"I-\",\"\").replace(\"B-\",\"\")\n",
        "        if token[\"label\"] != \"O\":\n",
        "\n",
        "            if label != prev_label:\n",
        "                ents.append({\"token\": [token[\"token\"]], \"label\": label})\n",
        "            else:\n",
        "                ents[-1][\"token\"].append(token[\"token\"])\n",
        "\n",
        "        prev_label = label\n",
        "\n",
        "    # group tokens\n",
        "    ents = [{\"token\": \"\".join(rec[\"token\"]).replace(start_token,\" \").strip(), \"label\": rec[\"label\"]}  for rec in ents ]\n",
        "\n",
        "    return ents"
      ],
      "metadata": {
        "id": "e53vqe2ENOHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **التجربة والنتيجة**"
      ],
      "metadata": {
        "id": "AONrOvWBW2Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [\"تعرض اللاعب محمد الشناوي لاعب النادي الأهلي لإصابة شديدة الخطورة، نقل على أثرها إلى المستشفى، لتلقي العلاج الطبي، لكن تعافى قبل أن يصل إلى المستشفى! في تاريخ 09/12/2023\"]\n",
        "\n",
        "# [optional]\n",
        "samples = [ \" \".join(word_tokenize(sample.strip())) for sample in samples if sample.strip() != \"\" ]\n",
        "\n",
        "for sample in samples:\n",
        "    ents = _extract_ner(text=sample, model=model, tokenizer=tokenizer, start_token=\"▁\")\n",
        "\n",
        "    print(sample)\n",
        "    for ent in ents:\n",
        "        print(\"\\t\",ent[\"token\"],\"==>\",ent[\"label\"])\n",
        "    print(\"========\\n\")"
      ],
      "metadata": {
        "id": "oa6OV3EYNSgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c147c5-f4e1-4de3-bc55-423e1173d8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تعرض اللاعب محمد الشناوي لاعب النادي الأهلي لإصابة شديدة الخطورة، نقل على أثرها إلى المستشفى، لتلقي العلاج الطبي، لكن تعافى قبل أن يصل إلى المستشفى ! في تاريخ 09/12/2023\n",
            "\t اللاعب ==> job\n",
            "\t محمد الشناوي ==> person\n",
            "\t لاعب ==> job\n",
            "\t النادي الأهلي ==> organization\n",
            "\t 09/12/2023 ==> time\n",
            "========\n",
            "\n"
          ]
        }
      ]
    }
  ]
}